[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\nThough this is a project for a data science class on machine learning, I hope to grow it into a documentation of my work with these data as a graduate student and perhaps even a discussion starter for other fiber photometry users. I hope this site comes across as a guide for beginners looking to try new things with photometry as well as an exploration of how modern data techniques and research can come together."
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Example Results",
    "section": "",
    "text": "Subject Training Validity Based on Model Results\n\nValid Subject\n\n\n\nSubject 1038 - Random Forest Model Visualization\n\n\nIn the figures above, it is clear that the model is able to predict responses in the final day of training with minimal error while explaining most of the variance in the data. In all models for this subject, maxStim was the most influential predictor of accuracy. This is reassuring, as this would translate to this subject’s responses to the cue being best predicted by the magnitude of the reward. Not only that, but this association may have even been learned as early as the first session, where maxStim was still the most influential predictor.\nResults and visualization from this model are in line with this subject’s averaged traces from training sessions, which are still the gold standard for fiber photometry data analysis. These traces are shown below, and as would be expected from a well-trained animal, responses are clean and discriminable.\n\n\n\nSubject 1038 - Averaged Traces for Each Trial Type Across Sessions\n\n\n\n\nInvalid Subject\n\n\n\nSubject 1075 - Random Forest Model Visualization\n\n\nIn this subject’s data, even by the last session of training, a random forest model was not able to explain any of the variance in the following day’s data. The plotted variable importance is less consistent and displays Trial number as the most influential predictor, which is theoretically agnostic to reward learning. Similarly, these plots reflect the lack of discrimination and general cue response in this subject’s averaged traces.\n\n\n\nSubject 1075 - Averaged Traces for Each Trial Type Across Sessions\n\n\n\n\nSummary Table\nAs mentioned, these data culminate into a summary table for a total of 24 subjects, all of which were exposed to the same training protocol. This serves as a quick way to go through and pick out subjects to exclude from analyses, a process that is usually done manually by visualization of traces. Now, with a bit of help from machine learning techniques, more legitimate quantification restraints can be placed on the subject pool.\nModel: meanQ ~ FreqType + ITI + BlockTrial + Trial + meanITI + maxStim + SDITI , Model Type: Random Forest , Training Method: oob , Number of Trees: 1000 , mtry = 7\n\n\n\nSubject\nR2\nRMSE\nMAE\nMIV\n\n\n\n\n1041\n0.3949578\n1.7729765\n1.4652645\nmaxStim\n\n\n1052\n0.5793458\n4.7512960\n4.0446574\nmaxStim\n\n\n1054\n0.7272169\n8.9439934\n7.7853564\nFreqTypeFrequency: 141\n\n\n1056\n0.5839287\n5.7939225\n4.5195827\nmaxStim\n\n\n1058\n0.5817878\n4.2655661\n3.2458450\nmaxStim\n\n\n1028\n0.8509219\n2.7488576\n2.1414415\nmaxStim\n\n\n1031\n0.6272406\n1.6812927\n1.3531908\nmaxStim\n\n\n1036\n0.6701905\n3.1273694\n2.6344605\nmaxStim\n\n\n1038\n0.7797942\n0.6435647\n0.4929969\nmaxStim\n\n\n1059\n0.2962061\n0.6654981\n0.5170113\nmaxStim\n\n\n1061\n0.3405076\n0.4895846\n0.4351899\nmaxStim\n\n\n1069\n0.1491278\n0.8778398\n0.6704789\nmaxStim\n\n\n1073\n0.0490711\n4.3076709\n3.3631842\nFreqTypeFrequency: 141\n\n\n1075\n0.0003382\n1.1213876\n0.8784168\nTrial\n\n\n1077\n0.3447976\n6.4837911\n5.2959359\nmaxStim\n\n\n1079\n0.0129195\n1.6052732\n1.1530791\nTrial\n\n\n1081\n0.3043969\n3.2576226\n2.7574021\nmeanITI\n\n\n1083\n0.0131632\n1.0648106\n0.8615191\nFreqTypeFrequency: 63\n\n\n1085\n0.1226153\n2.0922213\n1.6704706\nmaxStim\n\n\n1045\n0.1092476\n0.9239816\n0.7153494\nmeanITI\n\n\n1046\n0.6625300\n2.5399659\n1.9851904\nmaxStim\n\n\n1050\n0.4741522\n3.6576754\n2.9201575\nFreqTypeFrequency: 141\n\n\n1043\n0.4106178\n1.7732621\n1.3511310\nFreqTypeFrequency: 63\n\n\n1051\n0.1127578\n2.5399590\n2.1136078\nFreqTypeFrequency: 79"
  },
  {
    "objectID": "projectBackground.html",
    "href": "projectBackground.html",
    "title": "Project Background",
    "section": "",
    "text": "Learning Reward Discrimination in Purely Pavlovian Conditioning\nMy current projects involve training rats on a Pavlovian paradigm to discriminate between five different cues that predict five increasing levels of reward. In each trial, an auditory tone is played for three seconds, then a brief 500ms pulse of electrical stimulation is delivered to the medial forebrain bundle (MFB). Each of the five tones (ascending in pitch) is paired with a distinct brain stimulation reward (BSR), save for the lowest tone that is followed by no stimulation as a probe trial. Over one full session, these five trial types are presented ten times each in random order, totaling to 50 trials per session. Over ten sessions, animals’ signal recorded from dopamine neurons in the nucleus accumbens shows a learned, graded association between cue and stimulation.\n\n\n\nExample averaged traces of dopamine signal on the first and last day of training\n\n\n\nRationale\nEstablishing a stable learning paradigm around discriminable stimuli of increasing value allows me to ask questions about associative reward learning with much more nuance. Trained rats in various treatment conditions show altered responses to the cue or stimulation, giving further insight into the mechanisms treatment effects. Furthermore, in just the training data alone, we are now able to observe, quantify, and perhaps even model the time course of reward learning."
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Machine Learning Model",
    "section": "",
    "text": "Using Random Forest Models to Predict Training Stability\n\nCan a model from a previous training day predict the current day’s responses?\nMy aim is to determine whether it is reasonable to use a model predicting a future training day’s data as a way to determine that an animal has sufficiently learned an association. By doing so, I am treating the following day as the test data while the model describes the training data. I am using random forest modeling because it can handle smaller data sets (as within-subjects studies often yield) by training on bootstrapped data, it can fit to continuous dependent variables, and it protects dominant predictors from being overrepresented in the model.\nFor model fitness, I will be using R-squared to assess how much variance the model is able to account for as well as root mean squared error (RMSE) and mean absolute error (MAE) to ensure that model error stays within reasonable bounds.\nAll analysis coding is performed using R. All signal processing is performed in MatLab.\n    currModel &lt;- train(meanQ~FreqType+ITI+BlockTrial+Trial+meanITI+maxStim+SDITI,\n                data  = currData,\n                method = \"rf\",\n                trControl=trainControl(method = 'oob'), \n                preProcess=c(\"center\",\"scale\"),\n                metric='RMSE',\n                importance = TRUE,\n                ntree=1000,\n                tuneGrid = expand.grid(mtry=seq(1, 7,1)))\n\n\nInputs\nFrom my processed data (subtracted and filtered signal using PASTa toolbox), I have selected a number of features extracted from time-locked chunks of the session stream. These are defined as:\n\nmeanQ: The mean of all data points recorded within the 3-second cue period\nmaxQ: The peak value recorded during the 3-second cue period\nmeanStim: The mean of all data points recorded within the 5-second stimulation period\nmaxStim: The peak value recorded during the 5-second stimulation period\nITI: The length of time between the present and previous trials, randomized between 22-30 seconds\nMeanITI: The mean of all data points recorded within the ITI period\nSDITI: The variance during the ITI period\nFreqType: Which of the five cue+stimulation pairings was presented in that trial\nBlockTrial: The order of presentation within the “block” that the trial type arrives in. There are 10 blocks thorughout the session, and each frequency type is presented once during each block\nTrial: A value 1-50 that denotes the number of trials that have passed\n\nHere, meanQ is used as the dependent variable because I am most interested in the development of responses to the learned cue.\n\n\nOutputs\nThis analysis pipeline creates a model for each training day for each individual animal. It then calculates feature importance for each model, which is a measure that identifies the most influential predictors in model accuracy. Lastly, each day’s model is then used to predict its following day’s dataset. Each tested day’s model gets a measure of R-squared, RSME, and MAE. Importantly, as this is within-subjects, each model, its ranked variables, and its prediction of the following day get assigned a new object within the global environment so that no data is overwritten as the for-loop iterates through subjects.\n  for(currSession in 2:nSessions){\n    currData &lt;- currSubjectData %&gt;% filter(Day==currSession)\n    predictModelName &lt;- c(paste(currSubjectModels[currSession-1]))\n    predictModel &lt;- get(predictModelName)\n    currPredictTest &lt;- predictModel %&gt;% predict(currData)\n    currValidationTest &lt;-data.frame(R2.test = R2(currPredictTest, currData$meanQ),\n                                    RMSE.test = RMSE(currPredictTest, currData$meanQ),\n                                    MAE.test = MAE(currPredictTest, currData$meanQ),\n                                    validation.session = currSession)\n    currValidationTestName &lt;- c(paste(\"validationSetTest\", currSubject, currSession, sep=\".\"))\n    assign(currValidationTestName, currValidationTest)\n    currSubjectValidationNameList[currSession-1] &lt;- currValidationTestName}\nTo better visualize the output data, this analysis pipeline returns a layout of three plots: 1) RSME and MAE from each model predicting the following day across sessions, 2) R-squared from each model predicting the following day across sessions, and 3) variable importance from each model across sessions. This gives a better look into how predictors of dopamine response shift over time. These figures are saved to a specified path.\nLastly, the analysis pipeline compiles all subjects into one table that gives the R-squared, RMSE, MAE, and most influential variable (MIV) of the 9th model predicting the last day of training. These results are all compiled into a table.\nExamples of these outputs can be viewed under the results tab."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "A Second Pair of Eyes: Machine Learning in Fiber Photometry Data Analysis\nWelcome to my portfolio, a documentation of and guide to using machine learning techniques to assess fiber photometry data. As a relatively new method, fiber photometry offers researchers the ability to record population-level signal from neurons in awake and behaving animals in real time.\nA frontier in neural measurements, while exciting, comes with its challenges in validation. Readout of photometry data comes in the form of signal streams composed of over a thousand data points per second. Often, bits of these cropped streams will be time-locked to the onset of a stimulus, then averaged traces across trials represent neural responses.\n\n\n\nGraphic depicting averaged traces in photometry data collection\n\n\n\nWhat is really being analyzed?\nQuantification of these massive data streams requires manual feature selection at the hands of the experimenter. On top of this, the preprocessing (subtracting, filtering, scaling) of signal before analysis is yet to be standardized in the field. To take this even further, whether an animal has a clear signal has a large impact on the data, making between-animal validation a challenge. In sum, the potential for cherry-picking effects is high, and support from machine learning techniques may aid in reducing arbitration.\nCommon features selected in fiber photometry data focus on the frequency and amplitude of transients, or events that show a synchronized neural population response. Because a single-trial transient can be unstable or “noisy,” it is common practice to run multiple trials and average these measures. While the averaged data are indeed more stable, the time course of how these responses develop over a session is lost. When studying behavior, the time course of learning is too important to smooth over."
  }
]